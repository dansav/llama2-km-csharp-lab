{
  "Logging": {
    "LogLevel": {
      "Default": "Trace",
      // Examples: how to handle logs differently by class
      //      "Microsoft.KernelMemory.Handlers.TextExtractionHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.TextPartitioningHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.Handlers.SaveEmbeddingsHandler": "Information",
      //      "Microsoft.KernelMemory.ContentStorage.AzureBlobs": "Information",
      //      "Microsoft.KernelMemory.Pipeline.Queue.AzureQueues": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "KernelMemory": {
    "Services": {
      "LlamaSharp": {
        // path to file, e.g. "llama-2-7b-chat.Q6_K.gguf"
        "ModelPath": "E:\\test\\llama\\models\\llama-2-7b-chat.Q8_0.gguf",
        // Max number of tokens supported by the model
        "MaxTokenTotal": 4096,
        // Optional parameters
        "GpuLayerCount": 32
        // "Seed": 1337,
      }
    }
  }
}